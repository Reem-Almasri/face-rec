{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb8d24-976f-4327-8761-e3315637ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import time \n",
    "\n",
    "# Step 1: Capture Images for Multiple People\n",
    "\n",
    "def capture_images_for_person(name, num_images=20, img_size=(64, 64), delay=0.5):\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    os.makedirs(f\"dataset/{name}\", exist_ok=True)\n",
    "    print(f\"Capturing images for {name}...\")\n",
    "    \n",
    "    count = 0\n",
    "    while count < num_images:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video frame.\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face ROI and resize it\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face_roi, img_size)\n",
    "            \n",
    "            # Save the face image\n",
    "            image_path = f\"dataset/{name}/{name}_{count}.jpg\"\n",
    "            cv2.imwrite(image_path, face_resized)\n",
    "            count += 1\n",
    "            cv2.putText(frame, f\"Capturing {count}/{num_images} images\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            break\n",
    "\n",
    "        cv2.imshow(f\"Capture Images for {name}\", frame)\n",
    "\n",
    "        # Add a delay between captures\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    print(f\"Captured {count} images for {name}.\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with 'num_classes' neurons\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def load_data(dataset_dir, img_size=(64, 64)):\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    label_mapping = {}\n",
    "    current_label = 0\n",
    "\n",
    "    for person_name in os.listdir(dataset_dir):\n",
    "        person_dir = os.path.join(dataset_dir, person_name)\n",
    "        if os.path.isdir(person_dir):\n",
    "            label_mapping[current_label] = person_name\n",
    "            for img_name in os.listdir(person_dir):\n",
    "                img_path = os.path.join(person_dir, img_name)\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is not None:\n",
    "                    image_resized = cv2.resize(image, img_size) / 255.0\n",
    "                    images.append(image_resized)\n",
    "                    labels.append(current_label)\n",
    "            current_label += 1\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # One-hot encode labels\n",
    "    labels = np.eye(len(label_mapping))[labels]\n",
    "\n",
    "    return images, labels, label_mapping\n",
    "\n",
    "def train_model(dataset_dir, img_size=(64, 64)):\n",
    "    \n",
    "    images, labels, label_mapping = load_data(dataset_dir, img_size)\n",
    "    num_classes = len(label_mapping)\n",
    "\n",
    "    model = build_cnn(input_shape=(img_size[0], img_size[1], 3), num_classes=num_classes)\n",
    "    model.fit(images, labels, epochs=10, batch_size=8)\n",
    "\n",
    "    model.save(\"face_recognition_model.h5\")\n",
    "    np.save(\"label_mapping.npy\", label_mapping)\n",
    "\n",
    "    print(\"Model training complete. Saved model and label mapping.\")\n",
    "    return model, label_mapping\n",
    "\n",
    "# Step 3: Real-Time Face Recognition\n",
    "\n",
    "def recognize_faces(video_source=0, img_size=(64, 64)):\n",
    "\n",
    "    # Load the trained model and label mapping\n",
    "    from tensorflow.keras.models import load_model\n",
    "    model = load_model(\"face_recognition_model.h5\")\n",
    "    label_mapping = np.load(\"label_mapping.npy\", allow_pickle=True).item()\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "    print(\"Starting video stream... Press 'q' to quit.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video frame.\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face_roi, img_size) / 255.0\n",
    "            face_resized = face_resized.reshape(1, img_size[0], img_size[1], 3)\n",
    "\n",
    "\n",
    "            predictions = model.predict(face_resized)\n",
    "            predicted_label = np.argmax(predictions)\n",
    "            confidence = predictions[0][predicted_label]\n",
    "\n",
    "            if confidence > 0.6:  # Threshold for recognition\n",
    "                name = label_mapping[predicted_label]\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{name} ({confidence:.2f})\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2e305-41b8-4746-9d32-0dcd96682a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_images_for_person(\"Reem\", num_images=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad7e01-bda5-4815-8443-039e9a2ea516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture_images_for_person(\"name\", num_images=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f998c-ea25-4631-9fa8-98573ea734f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, label_mapping = train_model(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c0503-c5f9-47aa-9cfa-ab7f170ce175",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_faces()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
